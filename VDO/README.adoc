:scrollbar:
:data-uri:
:toc2:
:imagesdir: images

= VDO: The disk optimizer for your datacenter! - LTO Show & Tell Series

== Description
This Show&Tell is aimed to share some of the most important concepts and features of the Virtual Data optimizer data reduction layer.

Audience: IT Managers, Architects and technical staff who operates Linux

:numbered:

== Introduction

All new generation applications are producing and consuming a big amount of data every day, every hour and even every minute. 

Social media and The Internet of Things (IoT) are in this race to fill out lots and lots of disks globally. This enormous amount of data is susceptible to be analysed by Big Data and Artificial Intelligence algorithms to produce more swallowable information to get insights in order to produce actionable tasks and for understanding better and be more accurate in any aspect of this new information era.

The uncontrollable growth of this data needs to be stored in @traditional disks, that although every day is cheaper, they are not at the same pace of data production. This reality is in front of us and it is not going to stop, so we need to implement mechanisms for saving storage space to store more data in less physical space. 

In the old days, compression algorithms like zip or gzip were in place at the userland domain. Every file we wanted to shrink in space needed to be passed through an application in charge of compress it to produce a more storage optimized format. This needed to be triggered by applications or users on demand, so where not as practical as it may be believed.

So, something better is needed to insert in this equation. Something more automated and transparent to users or applications. That is why some techniques and mechanisms have come out to aim for the space savings goal.
That is the case with VDO. The virtual data optimizer is a kernel module that is in charge of applying optimization algorithms to data in order to reduce their disk occupation.

It works as an optimization layer between the disk driver and the filesystem, applying in real time algorithms of compression, deduplication and zero-block elimination to store a transformed data which occupy less space than the original source.

This way users or applications do not notice this intervention, so not needing to be informed or modified to deal with volumes now optimized. 

== Some Concepts to tune us well!

VDO sits on top of any block device and below the filesystem, so applications and users can transparently use the disk as usual without taking care about underlying optimization architecture in place.

That's why VDO is so convenient for putting in place optimization mechanisms that can be used for many use cases in the customer's organization.

VDO applies three different algorithms in a sequence for achieving great optimizations in the storage layer.

* *Zero-Block Elimination*: any block that consists exclusively of zeros is identified and recorded only in a special place for metadata. This means that VDO only store in physical disk Blocks that contain something other than zeros. This is the first thing VDO executes over the data analyzed.

* *Deduplication*: After the data is parsed and it is free of blocks of zeroes it is processed to determine whether it is redundant or not (data that has been written before). The redundancy of this data is checked through metadata maintained by the UDS (Universal Deduplication Service) kernel module which is a fundamental part of VDO. Any block of data that is found written before will not be written again, instead,  the metadata will be updated to point out to the original copy of the block already stored in disk.  

* *Compression*: Once these 2 phases are accomplished, LZ4 compression is applied to the remaining data blocks. The compressed blocks are then stored together into fixed length (4 KB) blocks and stored on disks.  There is a speed up optimization when a single physical block comprise many compressed blocks, in such a way readings can be benefited.

image::vdo-fig1.png[]

After this 3-phase process VDO can optimize data big time, but this optimization depends on the very nature of the data stored on disk, so special caution about in which cases VDO could be a good tool to have between the block device and the filesystem.

== VDO architecture and components 

The VDO solution consists of kvdo, uds and command line tools.

*The VDO Kernel Module (kvd)*

The kvdo Linux kernel module provides block-layer deduplication services within the Linux Device Mapper layer. In the Linux kernel, Device Mapper serves as a generic framework for managing pools of block storage, allowing the insertion of block-processing modules into the storage stack between the kernel's block interface and the actual storage device drivers.

The kvdo module is exposed as a block device that can be accessed directly for block storage or presented through one of the many available Linux file systems, such as XFS or ext4. 

When kvdo receives a request to read a (logical) block of data from a VDO volume, it maps the requested logical block to the underlying physical block and then reads and returns the requested data.

*The UDS Kernel Module (uds)*

The UDS index provides the foundation of the VDO product. For each new piece of data, it quickly determines if that piece is identical to any previously stored piece of data. If the index finds a match, the storage system can then internally reference the existing item to avoid storing the same information more than once.

The UDS index runs inside the kernel as the uds kernel module.

*Command line tools*

Tools like vdo and vdostats for configuring and managing optimized storage.

*What is a VDO Volume?*

VDO uses a block device as a storage, which can be made of an aggregation of physical storage consisting of one or more disks, partitions, or even flat files. When a VDO volume is created by a storage management tool, VDO reserves space from the volume for both a UDS index and the VDO volume, which interact together to provide deduplicated block storage to users and applications. 

image::vdo-fig2.png[]

*Slabs*

The physical storage of the VDO volume is divided into a number of slabs, each of which is a contiguous region of the physical space. All of the slabs for a given volume will be of the same size, which may be any power of 2 multiple of 128 MB up to 32 GB.

The default slab size is 2 GB. A single VDO volume may have up to 8096 slabs. Therefore, in the default configuration with 2 GB slabs, the maximum allowed physical storage is 16 TB. When using 32 GB slabs, the maximum allowed physical storage is 256 TB. At least one entire slab is reserved by VDO for metadata, and therefore cannot be used for storing user data.

Slab size has no effect on the performance of the VDO volume.

Recommended slap sizes:

image::vdo-fig3.png[]

*Physical storage*

A VDO volume is a thinly provisioned block device. It is best practice to place the volume on top of storage that can be expanded at a later time such as an LVM volume. 

A single VDO volume can be configured to use up to 256 TB of physical storage.

*Logical Size*

The logical volume size defaults to the available physical volume size. 

VDO currently supports any logical size up to 254 times the size of the physical volume with an absolute maximum logical size of 4PB.

*Write modes*

VDO supports two write modes: sync and async. When VDO is in sync mode, writes to the VDO device are acknowledged when the underlying storage has written the data permanently. When VDO is in async mode, writes are acknowledged before being written to persistent storage.

It is critical to set the VDO write policy to match the behavior of the underlying storage. By default, VDO write policy is set to the auto option, which selects the appropriate policy automatically.

== VDO Requirements

*RAM*

Each VDO volume has two distinct memory requirements:

* The VDO module requires 370 MB plus an additional 268 MB per each 1 TB of physical storage managed.

* The Universal Deduplication Service (UDS) index requires a minimum of 250 MB of DRAM, which is also the default amount that deduplication uses.  UDS uses an average of 4 bytes per entry in memory (including cache).

*Storage*

VDO requires storage for two types of data: metadata and UDS index:

* The first type of VDO metadata uses approximately 1 MB for each 4 GB of physical storage plus an additional 1 MB per slab.

* The second type of VDO metadata consumes approximately 1.25 MB for each 1 GB of logical storage, rounded up to the nearest slab.

* The amount of storage required for the UDS index depends on the type of index and the amount of RAM allocated to the index. For each 1 GB of RAM, a dense UDS index uses 17 GB of storage, and a sparse UDS index will use 170 GB of storage.

*Some examples of VDO Storage and Memory Requirements for Primary Storage*

image::vdo-fig4.png[]

*Some examples of VDO Storage and Memory Requirements for Backup Storage*

image::vdo-fig5.png[]

*Software*

VDO depends on the following software:

* LVM
* Python 2.7

== Common use cases 

VDO can be used for several use cases commonly present in the majority of enterprises nowadays. Some of the use cases have strong patterns that VDO exploits for maximizing the optimization process.

A VDO volume can store the following types of data that are good candidates for being optimized.

* VMs and Container
* Logs consolidation
* Session recordings
* Home Directories
* Backups

*Red Hat estimates some optimizations depending on the use case.*

When hosting active VMs or containers, Red Hat recommends provisioning storage at a 10:1 logical to physical ratio: that is, if you are utilizing 1 TB of physical storage, you would present it as 10 TB of logical storage.

For object storage, such as the type provided by Ceph, Red Hat recommends using a 3:1 logical to physical ratio: that is, 1 TB of physical storage would present as 3 TB logical storage.

In any case though, the optimization ratio is determined by the behavior and characteristics of the data stored, so it is very important to sit a VDO Volume on top a flexible volume like LVM for expanding space as needed.

== Deployment Scenarios

Because VDO exposes its deduplicated storage as a standard Linux block device, you can use it with standard file systems, iSCSI and FC target drivers, or as unified storage.
 
Some possible deployment scenarios are listed below:
 
*As a repository for VM disks*

image::vdo-fig6.png[]

*A shared file system* 

image::vdo-fig7.png[]

*An iSCSI Target*

image::vdo-fig8.png[]

*Multiple LVMs on an optimized volume*

image::vdo-fig9.png[]

*Encrypted Volume*

image::vdo-fig10.png[]

== So, let’s install VDO in the host and configure an optimized volume!
 
For using VDO, it is necessary to install the following packages:

[source, bash]
------------------
[root@server ~]# yum install vdo kmod-kvdo
------------------

Once installed, let's create a vdo volume on top of a RAID device. We have present 2 disks of 8GB: sdb, sdbc.
 
But first take a look at the block diagram to check what we are going to do with the command line.

image::vdo-fig11.png[]

[source, bash]
------------------
[root@server ~]#  mdadm --create --verbose /dev/md0 --level=mirror --raid-devices=2 /dev/sdb /dev/sdc

[root@server ~]#  watch cat /proc/mdstat 
------------------

After having a RAID device we need to create a volume group called vg1 on top of that, based on the best practice of having a volume management tool for adding space as needed.

[source, bash]
------------------
[root@server ~]# vgcreate vg1 /dev/md0
------------------

Then we need to create a Logical Volume called lv1 to have flexibility for future growth. The logical volume created is 4G of size, and it can grow up to 8G.

[source, bash]
------------------
[root@server ~]# lvcreate vg1 -L 4G -n lv1
------------------

Once this is created we have the underlying block device for creating the VDO volume called vdovolume. The logical size is going to be the double of the physical size configured which is 4G.


[source, bash]
------------------
[root@server ~]# vdo create --name=vdovolume --device=/dev/mapper/vg1-lv1 --vdoLogicalSize=8G --writePolicy=async --vdoSlabSize=512M

Creating VDO vdovolume
Starting VDO vdovolume
Starting compression on VDO vdovolume
VDO instance 5 volume is ready at /dev/mapper/vdovolume
------------------

After this we need to create a format and mount the device.

[source, bash]
------------------
[root@server ~]# mkfs.xfs /dev/mapper/vdovolume

meta-data=/dev/mapper/vdovolume  isize=512    agcount=4, agsize=524288 blks
         =                       sectsz=4096  attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=2097152, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=4096  sunit=1 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0

[root@server ~]# mkdir /data

[root@server ~]# mount /dev/mapper/vdovolume /data

[root@server ~]# df -kh /data
Filesystem             Size  Used Avail Use% Mounted 
/dev/mapper/vdovolume  8.0G   90M  8.0G   2% /data

[root@server ~]# vdostats --hu
Device                    Size      Used Available Use% Space saving%
/dev/mapper/vdovolume      4.0G      3.0G   1021.8M  75%           99%
------------------

If we want to persist the volume after a boot we need to create an entry on the fstab file in /etc.

[source, bash]
------------------
/dev/mapper/vdovolume /data auto _netdev,x-systemd.device-timeout=0,x-systemd.requires=vdo.service 0 0
------------------

The device is ready to be used as a regular disk. Also we can grow the volume using the growLogical parameter.

[source, bash]
------------------
[root@server ~]# vdo growLogical --name=vdovolume --vdoLogicalSize=20G

[root@server ~]# xfs_growfs /data
meta-data=/dev/mapper/vdovolume  isize=512    agcount=4, agsize=524288 blks
         =                       sectsz=4096  attr=2, projid32bit=1
         =                       crc=1        finobt=1, sparse=1, rmapbt=0
         =                       reflink=1
data     =                       bsize=4096   blocks=2097152, imaxpct=25
         =                       sunit=0      swidth=0 blks
naming   =version 2              bsize=4096   ascii-ci=0, ftype=1
log      =internal log           bsize=4096   blocks=2560, version=2
         =                       sectsz=4096  sunit=1 blks, lazy-count=1
realtime =none                   extsz=4096   blocks=0, rtextents=0
data blocks changed from 2097152 to 5242880

[root@server ~]# df -kh /data
Filesystem             Size  Used Avail Use% Mounted on
/dev/mapper/vdovolume   20G  176M   20G   1% /data
------------------

== A simulated scenario of optimization
 
When we mount a vdo volume, it is ready to be used and look for having the best optimization possible, but this depends on the nature of the data stored.
 
The script below can be used for creating a set of data that can simulate binary and text files in some variability fashion.

[source, bash]
------------------
#!/usr/bin/bash

home=/data
cat /dev/urandom | tr -dc "[:space:][:print:]" | head -c $(shuf -e 10000 100000 1000000 100000000 -n 1) > $home/binFile.bin;
cat /dev/urandom | tr -dc "[:space:][:print:]" | head -c $(shuf -e 10000 100000 1000000 100000000 -n 1) > $home/textFile.bin;

file0="/dev/null";
file=$home/$(date +'%Y%m%d_%H%M%S_%N'); 

ft0=$file0;
ft1=$file"_type1";
ft2=$file"_type2";
ft3=$file"_type3";
ft4=$file"_type4";

#higher the number of novar less variability
novarbin=$(shuf -i 3-50 -n 1)
novartext=$(shuf -i 3-50 -n 1)

var1=$(shuf -i 5-$novartext -n 1);
counter1=0;

var2=$(shuf -i 5-$novarbin -n 1);
counter2=0;

var3=$(shuf -i 5-10 -n 1);
counter3=0;

dataset=1;

while true; do 

	echo
	echo "Generating dataset: "$dataset
    echo "-----------------------"
    
	#files that are part similar and part diffent (TXT)
    echo -n $ft1; 
	cat /dev/urandom | tr -dc "[:space:][:print:]" | head -c $(shuf -e 500 1000 5000 10000 20000 100000 1000000 -n 1) > $ft1; 
	ps -ef >> $ft1; 
	journalctl -o verbose >> $ft1; 
	size=$(cat $ft1 | wc -c);
	echo -n "--> "$(numfmt --to=si $size);
	
	diff=$(diff $ft0 $ft1 | wc -c;)
	perc=$(echo "scale=2;100 - $diff/$(cat $ft1 | wc -c) * 100; scale=0"| bc)%
    echo "   diff: "${perc#-} 
	ft0=$ft1; 

	file=$home/$(date +'%Y%m%d_%H%M%S_%N');
	ft1=$file"_type1";

	#files that are part similar and part diffent (TXT+BIN)
	echo -n $ft2; 
	cat /dev/urandom | head -c $(shuf -e 500 1000 5000 10000 100000 -n 1) > $ft2; 
	size=$(cat $ft2 | wc -c);
	echo "--> "$(numfmt --to=si $size);
	ps -ef >> $ft2; 
	ft2=$file"_type2";

    #files that are equals (TXT)
	ft3=$file"_type3";
	/usr/bin/cp $home/textFile.bin $ft3;
	#~ echo $textFile > $ft3;
	size=$(cat $ft3 | wc -c);
	echo -n $ft3;
	echo "--> "$(numfmt --to=si $size);

	let counter1=counter1+1;
	#change content of file every var1 times
	if [[ $counter1 -gt $var1 ]]; then
	   echo
	   echo "------------------"
	   echo "changing textfile!";
	   echo "------------------"
	   echo
	   textFile=$(cat /dev/urandom | tr -dc "[:space:][:print:]" | head -c $(shuf -e 10000 100000 1000000 100000000 -n 1));
	   var1=$(shuf -i 5-$novartext -n 1)
       counter1=0
	fi


    #files that are equals (BIN)
	ft4=$file"_type4";
	/usr/bin/cp  $home/binFile.bin $ft4;
	size=$(cat $ft4 | wc -c);
	echo -n $ft4;
	echo "--> "$(numfmt --to=si $size);
	
	let counter2=counter2+1;
	#change content of file every var2 times
	if [[ $counter2 -gt $var2 ]]; then
	   echo
	   echo "------------------"
	   echo "changing binfile!";
	   echo "------------------"
	   echo
	   cat /dev/urandom | tr -dc "[:space:][:print:]" | head -c $(shuf -e 10000 100000 1000000 100000000 -n 1) > $home/binFile.bin;
	   var2=$(shuf -i 5-$novarbin -n 1)
       counter2=0
	fi

    #lets change the variability to simulate more or less users dinamically 
	let counter3=counter3+1;
	if [[ $counter3 -gt $var3 ]]; then
	   echo
	   echo "---------------------"
	   echo "changing variability!";
	   echo "---------------------"
	   echo
       novarbin=$(shuf -i 6-50 -n 1)
       novartext=$(shuf -i 6-50 -n 1)
       counter3=0
	fi

	
	let dataset=dataset+1

done
------------------

Copy & paste the above script to a file and change the permissions to executable. Then run it.. .please just be sure that the vdo volume is mounted in /data.
 
You can also copy & paste directly on to a terminal to execute it.
 
In another terminal accessing the same host you can execute the following script to evaluate the use of the disk and the optimization ratio reported by *vdstats*.

[source, bash]
------------------
clear; 
while true; do 
   df -kh /data; 
   echo ---;
   sudo vdostats --verbose | head -16; 
   echo “---”; 
   vdostats --hu ; 
   echo "---";ls /data | wc -l; 
   sleep 8; 
   clear; 
done
------------------

This escenario is simulated but can be used for showing how the optimization process transcur noticing the change in use of the logical and the physical disks. 

== Undoing the changes
 
What you’ll see in the following lines is a way to undo all the steps followed for creating the vdo volume, due to the fact that not only important to build but to de-construct.

[source, bash]
------------------
[root@server ~]# umount /data

[root@server ~]# vdo remove -n vdovolume
Removing VDO vdovolume
Stopping VDO vdovolume

[root@server ~]# lvremove /dev/mapper/vg1-lv1
Do you really want to remove active logical volume vg1/lv1? [y/n]: y
Logical volume "lv1" successfully removed

[root@server ~]#  vgremove /dev/mapper/vg1
Volume group "vg1" successfully removed

[root@server ~]# mdadm --stop /dev/md0
mdadm: stopped /dev/md0

[root@server ~]#  mdadm --zero-superblock /dev/sdb /dev/sdc
------------------

== Conclusions

VDO is a tool included in the operating system capable of saving space for regular activities like doing backups, consolidate logs or store virtual machines, which are duplicated in nature.

Although this optimization layer could ask for more resources of CPU and will not have the same performance as if it were in a plain disk, with solid state disks implemented, this penalty could be negligible.

For non critical data, this is a must to have, so we can use the saved space for other developments aiming to the enterprise digital transformation projects.

For critical data, still the need for a robust disk array is on the table.

== Some good sources of information

* https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-quick-start[Getting started with VDO]

* https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/pdf/deduplicating_and_compressing_storage/Red_Hat_Enterprise_Linux-8-Deduplicating_and_compressing_storage-en-US.pdf [Deduplicating and compressing storage]

* https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-ig-administering-vdo[Administering VDO]

* https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/vdo-ig-tuning-vdo[Tuning VDO]

