:scrollbar:
:data-uri:
:toc2:
:imagesdir: images

= High Availability add-on on RHEL - LTO Show & Tell Series

== Description
This Show&Tell is aimed to share some of the most important concepts and features of the High Availability posibilities RHEL propose.

Audience: IT Managers, Architects and technical staff who operates Linux

:numbered:

== Introduction

The High Availability Add-On is a clustered system that provides reliability, scalability, and availability to critical production services.

A cluster is two or more computers (called nodes or members) that work together to perform a task. Clusters can be used to provide highly available services or resources. The redundancy of multiple machines is used to guard against failures of many types.

== Components

Mayor components are listed below:

* Cluster infrastructure — Provides fundamental functions for nodes to work together as a cluster: configuration file management, membership management, lock management, and fencing.
* High availability service management — Provides failover of services from one cluster node to another in case a node becomes inoperative.
* Cluster administration tools — Configuration and management tools for setting up, configuring, and managing the High Availability Add-On. The tools are for use with the cluster infrastructure components, the high availability and service management components, and storage.

Some supplements can be used for the High Availability Add-On with the following components:

* Red Hat GFS2 (Global File System 2) — Part of the Resilient Storage Add-On, this provides a cluster file system for use with the High Availability Add-On. GFS2 allows multiple nodes to share storage at a block level as if the storage were connected locally to each cluster node. GFS2 cluster file system requires a cluster infrastructure.
* LVM Locking Daemon (lvmlockd) — Part of the Resilient Storage Add-On, this provides volume management of cluster storage. lvmlockd support also requires cluster infrastructure.
* Load Balancer Add-On — Routing software that provides high availability load balancing and failover in layer 4 (TCP) and layer 7 (HTTP, HTTPS) services. The Load Balancer Add-On runs in a cluster of redundant virtual routers that uses load algorithms to distribute client requests to real servers, collectively acting as a virtual server. It is not necessary to use the Load Balancer Add-On in conjunction with Pacemaker.

=== Pacemaker

Pacemaker is a cluster resource manager. It achieves maximum availability for our cluster services and resources by making use of the cluster infrastructure’s messaging and membership capabilities to deter and recover from node and resource-level failure.

==== Pacemaker components

* Cluster Information Base (CIB)
The Pacemaker information daemon, which uses XML internally to distribute and synchronize current configuration and status information from the Designated Coordinator (DC) — a node assigned by Pacemaker to store and distribute cluster state and actions by means of the CIB — to all other cluster nodes.

* Cluster Resource Management Daemon (CRMd)
Pacemaker cluster resource actions are routed through this daemon. Resources managed by CRMd can be queried by client systems, moved, instantiated, and changed when needed.

Each cluster node also includes a local resource manager daemon (LRMd) that acts as an interface between CRMd and resources. LRMd passes commands from CRMd to agents, such as starting and stopping and relaying status information.

* Shoot the Other Node in the Head (STONITH)
STONITH is the Pacemaker fencing implementation. It acts as a cluster resource in Pacemaker that processes fence requests, forcefully shutting down nodes and removing them from the cluster to ensure data integrity. STONITH is configured in the CIB and can be monitored as a normal cluster resource. 

* corosync
corosync is the component - and a daemon of the same name - that serves the core membership and member-communication needs for high availability clusters. It is required for the High Availability Add-On to function.

Other corosync functions are:

- Manages quorum rules and determination.
- Provides messaging capabilities for applications that coordinate or operate across multiple members of the cluster and thus must communicate stateful or other information - between instances.
- Uses the kronosnet library as its network transport to provide multiple redundant links and automatic failover.

=== The fencing mechanism

If communication with a single node in the cluster fails, then other nodes in the cluster must be able to restrict or release access to resources that the failed cluster node may have access to. Because the node itself may not be responsive this cannot be accomplished by contacting the cluster failed node. Instead, we must provide an external method, which is called fencing with a fence agent. A fence device is an external device that can be used by the cluster to restrict access to shared resources by an errant node, or to issue a hard reboot on the cluster node.

=== The Quorum Mechanism

In order to maintain cluster integrity and availability, cluster systems use a concept known as quorum to prevent data corruption and loss. A cluster has quorum when more than half of the cluster nodes are online. To mitigate the chance of data corruption due to failure, Pacemaker by default stops all resources if the cluster does not have quorum.

Quorum is established using a voting system. When a cluster node does not function as it should or loses communication with the rest of the cluster, the majority working nodes can vote to isolate and, if needed, fence the node for servicing.

The quorum features in Pacemaker prevent what is also known as split-brain, a phenomenon where the cluster is separated from communication but each part continues working as separate clusters, potentially writing to the same data and possibly causing corruption or loss. 

A Red Hat Enterprise Linux High Availability Add-On cluster uses the votequorum service, in conjunction with fencing, to avoid split brain situations. A number of votes is assigned to each system in the cluster, and cluster operations are allowed to proceed only when a majority of votes is present.

=== What is a Resource?

An instance of program, data, or application to be managed by the cluster service is called A cluster resource. Agents abtracts these resources providing a standard interface for managing the resource in a cluster environment.

We can add monitoring operations to a resource’s definition to ensure that resources remain healthy. 

The behavior of a resource in a cluster can be determined by configuring constraints. 

* location constraints — A location constraint determines which nodes a resource can run on.
* ordering constraints — An ordering constraint determines the order in which the resources run.
* colocation constraints — A colocation constraint determines where resources will be placed relative to other resources.

Also, groups can be configured for setting resources that need to be located together, start sequentially, and stop in the reverse order.

* Logical volumes 

The Red Hat High Availability Add-On provides support for LVM volumes in two distinct cluster configurations:

* High availability LVM volumes (HA-LVM) in active/passive failover configurations in which only a single node of the cluster accesses the storage at any one time.
* LVM volumes that use the lvmlockd daemon to manage storage devices in active/active configurations in which more than one node of the cluster requires access to the storage at the same time. The lvmlockd daemon is part of the Resilient Storage Add-On.

=== A general diagram of a cluster implemented

image::cluster_depicted.png[]

== A practical HA implementation

In the next procedure we are going implement a simple web service on 2 nodes. This service will be in charge to serve a web page with apache and the content of this page is going to be stored in a shared iscsi storage. Simple, no?... let's get our hands dirty.

=== Some conventions for the rest of this document.

Let's assume the following server layout.

ha1.test.com - node 1
ha2.test.com - node 2
str.test.com - iscsi target

Every procedure explained in the next sections will have a scope of action that we are going describe with a tag and the scope. 

TAG [scope]

For example if we need to execute a command or procedure on ha1 server, the scope should look like this.

In which servers[ha1]

Or, if we need to execute a command on all servers of the cluster the tag should be represented by:

In which server[cluster]

Where cluster is comprised of ha1.test.com, ha2.test.com

In which servers[ALL]

In this case we consider the nodes of the cluster + the storage server.

So, you get my idea of the scoping.

=== Creating trust among servers
In which servers: [ALL]

Let's create a circle of trust for having SSH passwordless channels between servers.

As root, let's get access to each server and execute the following sequence.

In each server execute the following sequence.

[source, bash]
------------------------
ssh-keygen
for node in ha1 ha2 str; do ssh-copy-id $node; done
------------------------

=== Defining the naming convention in /etc/hosts
In which nodes: [ALL]

Change the IPs accordingly your servers are configured. 

Logged-in in the str.test.com server we can configure /etc/hosts as follow. First modify /etc/host in str.test.com.

[source, bash]
------------------------
cat <<EOF >> /etc/hosts
192.168.56.154 ha1.test.com ha1
192.168.56.160 ha2.test.com ha2
192.168.56.157 str.test.com str
EOF
------------------------

Then, modify all the cluster node members accessing every node:

[source, bash]
------------------------
ssh ha1

cat <<EOF >> /etc/hosts 
192.168.56.154 ha1.test.com ha1 
192.168.56.160 ha2.test.com ha2 
192.168.56.157 str.test.com str 
EOF

ssh ha2
...
------------------------

We can chack what we just do it by executing the following from str:
[source, bash]
------------------------
for node in ha1 ha2; do \
echo $node; \
ssh $node cat /etc/hosts; \
done
------------------------

=== Configure the iscsi target server

We need an entity that shares a storage for this excercise. One easy way to do it is setting a server to serve an iscsi volume, which precisely is what we are going to do in the following simple steps.

So, our server is called str.test.com. Is a regular RHEL 8.2 server installed with all defaults. We have to install the software needed for enable this node to serve a volume using the iscsi protocol.

==== Installing the software
In which server[str.test.com]

Get access to the server via ssh then become root or use "sudo" for all the commands that are described below.

[source, bash]
------------------------
yum install -y targetcli


Updating Subscription Management repositories.
Red Hat Enterprise Linux 8 for x86_64 - AppStream (RPMs)                         726 kB/s |  19 MB     00:27
Red Hat Enterprise Linux 8 for x86_64 - BaseOS (RPMs)                            815 kB/s |  22 MB     00:27
Dependencies resolved.
=================================================================================================================
 Package                    Architecture  Version                  Repository                               Size
=================================================================================================================
Installing:
 targetcli                  noarch        2.1.51-4.el8_2           rhel-8-for-x86_64-appstream-rpms         79 k
Installing dependencies:
 python3-configshell        noarch        1:1.1.27-1.el8           rhel-8-for-x86_64-baseos-rpms            74 k
 python3-kmod               x86_64        0.9-20.el8               rhel-8-for-x86_64-baseos-rpms            90 k
 python3-pyparsing          noarch        2.1.10-7.el8             rhel-8-for-x86_64-baseos-rpms           142 k
 python3-rtslib             noarch        2.1.71-4.el8             rhel-8-for-x86_64-baseos-rpms           101 k
 python3-urwid              x86_64        1.3.1-4.el8              rhel-8-for-x86_64-baseos-rpms           783 k
 target-restore             noarch        2.1.71-4.el8             rhel-8-for-x86_64-baseos-rpms            24 k

Transaction Summary
=================================================================================================================
Install  7 Packages
...
Complete!
------------------------

==== Discovering iqns cluster nodes
In which server: [cluster]

Once we have the software installed we need to know which IQN have each of the cluster members. For this we only need to show the content of the initiatorname.iscsi files.

Having created the circle of trust, from str.test.com server we can get the IQN from each server as follow:

[source, bash]
------------------------
for node in ha1 ha2 ha3; do echo -n $node: $(ssh $node cat /etc/iscsi/initiatorname.iscsi); echo; done

ha1: InitiatorName=iqn.1994-05.com.redhat:9b97b0b38c9f
ha2: InitiatorName=iqn.1994-05.com.redhat:9b97b0b38c9e
------------------------

Take note on this. Every server must have the last hexadecimal part diferent from each other. If not, we have to generate a unique number on this.

==== Configuring ISCSI TARGET
In which server: [str.test.com]

Our str.text.con server have a disk in /dev/sdb which has 16GB in space.

Let's start the targetcli command for configuring the target and which should have access to the shared iscsi volume. 

[source, bash]
------------------------
targetcli
...

/> cd /backstores/block
/backstores/block> create iscsi_shared_storage /dev/sdb
Created block storage object iscsi_shared_storage using /dev/mapper/vdo

/backstores/block> cd /iscsi
/iscsi> create

Created target  iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172
Created TPG 1.
Global pref auto_add_default_portal=true
Created default portal listening on all IPs (0.0.0.0), port 3260.

/iscsi> cd iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172/tpg1/acls  << Change as per the output of previous command
/iscsi/iqn.20...e18/tpg1/acls> create iqn.1994-05.com.redhat:9b97b0b38c9f  << ha1
Created Node ACL for iqn.1994-05.com.redhat:9b97b0b38c9f
/iscsi/iqn.20...e18/tpg1/acls> create iqn.1994-05.com.redhat:9b97b0b38c9e  << ha2
Created Node ACL for iqn.1994-05.com.redhat:9b97b0b38c9e
/iscsi/iqn.20...e18/tpg1/acls> cd /iscsi/iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172/tpg1/luns
/iscsi/iqn.20...e18/tpg1/luns> create /backstores/block/iscsi_shared_storage

Created LUN 0.
Created LUN 0->0 mapping in node ACL iqn.1994-05.com.redhat:9b97b0b38c9f
Created LUN 0->0 mapping in node ACL iqn.1994-05.com.redhat:9b97b0b38c9e

/iscsi/iqn.20...e18/tpg1/luns> cd /
...
/> saveconfig
Configuration saved to /etc/target/saveconfig.json
/> exit
Global pref auto_save_on_exit=true
Last 10 configs saved in /etc/target/backup/.
Configuration saved to /etc/target/saveconfig.json
------------------------

==== Enabling iscsi services
In which server: [str]

After configuring the target it is time to activate the service as follow:

[source, bash]
------------------------
systemctl enable --now target
systemctl restart target
------------------------

==== Enabling the firewall port 
In which server: [str]

We need to enabling the port that by default pertain to the target service.

[source, bash]
------------------------
firewall-cmd --permanent --add-port=3260/tcp
firewall-cmd --reload
------------------------

So far we only have configured the server which is going to share the disk for the cluster. This could have been a storage array by itself. For the purpose of this excercise the iscsi protocol is enough.

=== Preparing the base for the Cluster

Our next step defines de base of our cluster which will have 3 nodes.

==== Enabling the proper repositories
In which servers: [cluster]

Check in which pool exists HA and resilient storage. Take note of the pool id.

[source, bash]
------------------------
subscription-manager list --available
------------------------

Then attach the corresponding pool and enable the proper repositories. This is needed for install the software required for this exercise.

[source, bash]
------------------------
for node in ha1 ha2; do
echo ------------------------------
echo Enabling repositories on node $node
ssh $node subscription-manager attach  --pool=8a85f99b727637b201729983324d0540
ssh $node subscription-manager repos --enable=rhel-8-for-x86_64-highavailability-rpms
ssh $node subscription-manager repos --enable=rhel-8-for-x86_64-resilientstorage-rpms
done
------------------------

==== Installing Software
In which servers: [cluster]

[source, bash]
------------------------
for node in ha1 ha2; do
echo ------------------------------
echo Installing software on node $node
ssh $node yum install -y pcs fence-agents-all pcp-zeroconf iscsi-initiator-utils lvm2-lockd gfs2-utils dlm
echo
done
------------------------

==== Enabling the Firewall
In which servers: [cluster]

[source, bash]
------------------------
for node in ha1 ha2; do 
echo Setting firewall on $node 
ssh $node systemctl enable --now firewalld 
ssh $node firewall-cmd --permanent --add-service=high-availability 
ssh $node firewall-cmd --add-service=high-availability 
ssh $node firewall-cmd --reload 
done
------------------------

Then we can check is the ha service is enabled.

[source, bash]
------------------------
for node in ha1 ha2; do
echo Services enabled on $node: $(ssh $node firewall-cmd --list-services)
done

Services enabled on ha1: cockpit dhcpv6-client high-availability ssh
Services enabled on ha2: cockpit dhcpv6-client high-availability ssh
------------------------

==== Set password for hacluster user
In which servers: [cluster]

We must define a password for administering the cluster with the hacluster user. For this exercise I am assigning "ltodemos" como password.

[source, bash]
------------------------
for node in ha1 ha2; do
echo Enter the password for $node
ssh $node passwd hacluster
echo
done
------------------------

==== Configuring the Cluster Service to be enabled and starts automatically at boot
In which servers: [cluster]

[source, bash]
------------------------
for node in ha1 ha2; do
echo enabling pcsd on $node
ssh $node systemctl enable --now pcsd
echo
done
------------------------

Then check the service on each node.

[source, bash]
------------------------
for node in ha1 ha2; do
echo $node ----------------
ssh $node systemctl status pcsd | sed ''/active/s//$(printf "\033[32mactive\033[0m")/''
echo
done
------------------------

=== Creating a high availability cluster

At this point we have installed the cluster software and enabled all the services needed for what comes next. We are going to create the cluster itself with the 3 nodes.

From now on, we are going to use cluster commands like "pcs" which only need to be issued from one of the cluster members.

==== Authenticating the pcs user hacluster for each node
In which servers: [ha1.test.com]

Use the password ltodemos configured in previous steps.

[source, bash]
------------------------
pcs host auth ha1.test.com ha2.test.com -u hacluster -p ltodemos

ha1.test.com: Authorized
ha2.test.com: Authorized
------------------------

==== Creating the 2-node cluster named "apache_cluster"
In which servers: [ha1]

[source, bash]
------------------------
pcs cluster setup apache_cluster --start ha1.test.com ha2.test.com

No addresses specified for host 'ha1.test.com', using 'ha1.test.com'
No addresses specified for host 'ha2.test.com', using 'ha2.test.com'
Destroying cluster on hosts: 'ha1.test.com', 'ha2.test.com'...
ha1.test.com: Successfully destroyed cluster
ha2.test.com: Successfully destroyed cluster
Requesting remove 'pcsd settings' from 'ha1.test.com', 'ha2.test.com'
ha1.test.com: successful removal of the file 'pcsd settings'
ha2.test.com: successful removal of the file 'pcsd settings'
Sending 'corosync authkey', 'pacemaker authkey' to 'ha1.test.com', 'ha2.test.com'
ha2.test.com: successful distribution of the file 'corosync authkey'
ha2.test.com: successful distribution of the file 'pacemaker authkey'
ha1.test.com: successful distribution of the file 'corosync authkey'
ha1.test.com: successful distribution of the file 'pacemaker authkey'
Sending 'corosync.conf' to 'ha1.test.com', 'ha2.test.com', 'ha3.test.com'
ha1.test.com: successful distribution of the file 'corosync.conf'
ha2.test.com: successful distribution of the file 'corosync.conf'
Cluster has been successfully set up.
Starting cluster on hosts: 'ha1.test.com', 'ha2.test.com'...
------------------------

==== Enabling the cluster to start on boot
In which servers: [ha1]

[source, bash]
------------------------
pcs cluster enable --all

ha1.test.com: Cluster Enabled
ha2.test.com: Cluster Enabled
------------------------

==== Checking the configuration so far

Let´s see if everything is working as expected.

[source, bash]
------------------------
pcs status | sed ''/active/s//$(printf "\033[32mactive\033[0m")/''

Cluster name: apache_cluster

WARNINGS:
No stonith devices and stonith-enabled is not false

Cluster Summary:
  * Stack: corosync
  * Current DC: ha1.test.com (version 2.0.3-5.el8_2.1-4b1f869f0f) - partition with quorum
  * Last updated: Wed Oct 21 10:39:33 2020
  * Last change:  Wed Oct 21 10:38:23 2020 by hacluster via crmd on ha1.test.com
  * 3 nodes configured
  * 0 resource instances configured

Node List:
  * Online: [ ha1.test.com ha2.test.com ]

Full List of Resources:
  * No resources

Daemon Status:
  corosync: active/enabled
  pacemaker: active/enabled
  pcsd: active/enabled

------------------------

Look at the sed part of the of the pcs command. This is only to show the "active" work in green. As we can see, corosync, pacemaker and pcsd are active.

=== Configuring a shared storage in the cluster

For the purpose of this exercise, we are going to use the iscsi server to present a common volume to all 3 nodes that are going to be members of the cluster that we just built. 

In this volume all nodes will have the possibility to read and write in sync and controlled by the cluster.

==== Configuring iscsi in all nodes
In which servers: [cluster]

You can execute this from any server. 

[source, bash]
------------------------
for node in ha1 ha2; do
echo Setting iscsi volume on node $node
ssh $node iscsiadm -m discovery -t sendtargets -p str.test.com
ssh $node iscsiadm -m node -L automatic
echo
done

Setting iscsi volume on node ha1
192.168.56.157:3260,1 iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172, portal: 192.168.56.157,3260]
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172, portal: 192.168.56.157,3260] successful.

Setting iscsi volume on node ha2
192.168.56.157:3260,1 iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172
Logging in to [iface: default, target: iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172, portal: 192.168.56.157,3260]
Login to [iface: default, target: iqn.2003-01.org.linux-iscsi.str.x8664:sn.cc4faab82172, portal: 192.168.56.157,3260] successful.
------------------------

==== Check for the disks presented in each node

[source, bash]
------------------------
for node in ha1 ha2; do
echo node $node: $(ssh $node lsblk | grep sdb)
done

node ha1: sdb 8:16 0 16G 0 disk
node ha2: sdb 8:16 0 16G 0 disk
------------------------

==== Configuring Shut The Other Node In The Head (STONITH)
In which servers: [ha1]

The iscsi target has been mapped on /dev/sdb and in this exercise is 16G size.

[source, bash]
------------------------
pcs stonith create scsi-shooter fence_scsi pcmk_host_list="ha1.test.com ha2.test.com" devices=/dev/sdb  meta provides=unfencing
pcs property set stonith-enabled=true
pcs property set no-quorum-policy=freeze
------------------------

Now check for the configuration made.

[source, bash]
------------------------
pcs stonith config scsi-shooter
------------------------

==== Setting up the disk for storing data

On both nodes of the cluster, perform the following steps to set the value for the LVM system ID to the value of the uname identifier for the system. The LVM system ID will be used to ensure that only the cluster is capable of activating the volume group.

Set the system_id_source configuration option in the */etc/lvm/lvm.conf* configuration file to uname.

In which servers: [cluster]
[source, bash]
------------------------
system_id_source = "uname"
------------------------

Verify that the LVM system ID on the node matches the uname for the node.

[source, bash]
------------------------
for node in ha1 ha2; do
ssh $node lvm systemid
ssh $node uname -n
done

system ID: ha1
ha1
system ID: ha2
ha2
------------------------

Create a physical volume on the first partition of /dev/sdb.

In which servers: [ha1]

[source, bash]
------------------------
pvcreate /dev/sdb1

Physical volume "/dev/sdb1" successfully created.
------------------------

Now we will create a Volume Group for in later time create a logical volume.

[source, bash]
------------------------
vgcreate apache_volume_group /dev/sdb1

Volume group "apache_volume_group" successfully created
------------------------

Let's see what we have created so far

[source, bash]
------------------------
vgs -o+systemid
 
  VG                  #PV #LV #SN Attr   VSize    VFree    System ID
  apache_volume_group   1   0   0 wz--n- <128.00g <128.00g
  rhel                  1   2   0 wz--n-  <15.00g       0
------------------------

Now it is time to create de logical volume.

[source, bash]
------------------------
lvcreate -L10G -n apache_data_volume apache_volume_group

  Logical volume "apache_data_volume" created.
------------------------

Then check it.

[source, bash]
------------------------
lvs

  LV                 VG                  Attr       LSize  Pool Origin Data%  Meta%  Move Log Cpy%Sync Convert
  apache_data_volume apache_volume_group -wi-a----- 10.00g
  root               rhel                -wi-ao---- 13.39g
  swap               rhel                -wi-ao----  1.60g
------------------------

Let's format the volume with ext4.

[source, bash]
------------------------
mkfs.ext4 /dev/apache_volume_group/apache_data_volume

mke2fs 1.45.4 (23-Sep-2019)
Creating filesystem with 2621440 4k blocks and 655360 inodes
Filesystem UUID: 7bdc13d4-9bde-467e-9fbe-27d62b26c975
Superblock backups stored on blocks:
        32768, 98304, 163840, 229376, 294912, 819200, 884736, 1605632

Allocating group tables: done
Writing inode tables: done
Creating journal (16384 blocks): done
Writing superblocks and filesystem accounting information: done
------------------------

==== Configuring APACHE
In which servers: [cluster]

[source, bash]
------------------------
for node in ha1 ha2; do
ssh $node yum install -y httpd wget
done
------------------------

In order for the Apache resource agent to get the status of the Apache HTTP Server, ensure that the following text is present in the /etc/httpd/conf/httpd.conf file on each node in the cluster, and ensure that it has not been commented out. If this text is not already present, add the text to the end of the file.

In each node we can configure this.

[source, bash]
------------------------
cat <<EOF >> /etc/httpd/conf/httpd.conf
<Location /server-status>
    SetHandler server-status
    Require local
</Location>
EOF
------------------------

When you use the apache resource agent to manage Apache, it does not use systemd. Because of this, you must edit the logrotate script supplied with Apache so that it does not use systemctl to reload Apache.

Remove the following line in the /etc/logrotate.d/httpd file on each node in the cluster.

[source, bash]
------------------------
/bin/systemctl reload httpd.service > /dev/null 2>/dev/null || true
------------------------

Replace the line you removed with the following three lines.

[source, bash]
------------------------
      /usr/bin/test -f /var/run/httpd.pid >/dev/null 2>/dev/null &&
      /usr/bin/ps -q /usr/bin/cat /var/run/httpd.pid >/dev/null 2>/dev/null &&
      /usr/sbin/httpd -f /etc/httpd/conf/httpd.conf -c "PidFile /var/run/httpd.pid" -k graceful > /dev/null 2>/dev/null || true
------------------------

Now let's create a simple web page and directory layout on our disk.

[source, bash]
------------------------
mount /dev/apache_volume_group/apache_data_volume /var/www
mkdir /var/www/html
mkdir /var/www/cgi-bin
mkdir /var/www/error
restorecon -R /var/www


cat <<-END >/var/www/html/index.html
<html>
<head>
<title>LATAM Technology Office</title>
</head>
<body>
<h1>Show & Tell is a Technical session oriented to RHEL. The goal is to teach how Cool is RHEL for any situation, platform and criticity</h1>
<p>Today is time to talk a little bit about how RHEL can provide a strong plataform for critical applications</p>
<p>High Availability in RHEL is very simple to implement: </p>
<ul>
<li>Straightforward</li>
<li>Secure</li>
<li>Scalable</li>
</ul>
<p><a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_high_availability_clusters/">Configuring HA in RHEL 8</a></p>
<hr>
<p><img src="https://i0.wp.com/www.itwarelatam.com/wp-content/uploads/2019/05/Red-Hat-696x418.jpg?fit=696%2C418&ssl=1" alt="A Great HTML Resource" width="10%" height="15%"><p>
<p>For more information check this link <a href="www.redhat.com">RED HAT</a></p>
<p>&#169; LATAM office of technology, 2020</p>
</body>
</html>
END

umount /var/www
------------------------

==== Creating resources and group of resources
In which servers: [ha1]

The following creates a resource calles apache_lvm which take cares of apache_volume_group and also creates the group called apachegroup and group this resource in that newly created group.

[source, bash]
------------------------
pcs resource create apache_lvm ocf:heartbeat:LVM-activate vgname=apache_volume_group vg_access_mode=system_id --group apachegroup
------------------------

The following commands create the remaining resources for the configuration, adding them to the existing resource group apachegroup.

The filesystem to be mounted on the active node which is going to be mounted on /var/www.

[source, bash]
------------------------
pcs resource create ha_apache_fs Filesystem \
device="/dev/apache_volume_group/apache_data_volume" directory="/var/www" fstype="ext4" \
--group apachegroup
------------------------

The virtual IP Address is going to be configured by the cluster in the active node as a secundary ip.

[source, bash]
------------------------
pcs resource create apache_virtualIP IPaddr2 ip=192.168.56.40 \
cidr_netmask=24 --group apachegroup
------------------------

The web service itself is managed by the apache agent.

[source, bash]
------------------------
pcs resource create new_app apache \
configfile="/etc/httpd/conf/httpd.conf" \
statusurl="http://127.0.0.1/server-status" --group apachegroup
------------------------

Config the firewall for accepting a regular web page traffic.

[source, bash]
------------------------
firewall-cmd --permanent --add-port=80/tcp
firewall-cmd --add-port=80/tcp
------------------------

==== Moving around resources

It is possible to migrate resources from one node to other using some features of pacemaker.

Let´s see where is our resource group apachegroup active.

[source, bash]
------------------------
pcs cluster status

Cluster Status:
 Cluster Summary:
   * Stack: corosync
   * Current DC: ha2.test.com (version 2.0.3-5.el8_2.1-4b1f869f0f) - partition with quorum
   * Last updated: Fri Oct 23 05:52:10 2020
   * Last change:  Thu Oct 22 18:38:22 2020 by root via crm_resource on ha1.test.com
   * 2 nodes configured
   * 5 resource instances configured
 Node List:
   * Online: [ ha1.test.com ha2.test.com ]

PCSD Status:
  ha2.test.com: Online
  ha1.test.com: Online
------------------------

We can see where a resource or resource group is being serving.

[source, bash]
------------------------
pcs resource status

  * Resource Group: apachegroup:
    * apache_lvm        (ocf::heartbeat:LVM-activate):  Started ha2.test.com
    * ha_apache_fs      (ocf::heartbeat:Filesystem):    Started ha2.test.com
    * apache_virtualIP  (ocf::heartbeat:IPaddr2):       Started ha2.test.com
    * new_app   (ocf::heartbeat:apache):        Started ha2.test.com
------------------------


Let's move the resourcegroup apachegroup from node ha2 to ha1.

[source, bash]
------------------------
pcs resource move apachegroup ha1.test.com

pcs resource status
  * Resource Group: apachegroup:
    * apache_lvm        (ocf::heartbeat:LVM-activate):  Started ha1.test.com
    * ha_apache_fs      (ocf::heartbeat:Filesystem):    Started ha1.test.com
    * apache_virtualIP  (ocf::heartbeat:IPaddr2):       Started ha1.test.com
    * new_app   (ocf::heartbeat:apache):        Started ha1.test.com
------------------------

We can disable a resource for start at boot.

[source, bash]
------------------------
pcs resource sdisable apachegroup

pcs resource status
  * Resource Group: apachegroup:
    * apache_lvm        (ocf::heartbeat:LVM-activate):  Stopped (disabled)
    * ha_apache_fs      (ocf::heartbeat:Filesystem):    Stopped (disabled)
    * apache_virtualIP  (ocf::heartbeat:IPaddr2):       Stopped (disabled)
    * new_app   (ocf::heartbeat:apache):        Stopped (disabled)
------------------------

To enable back the resouce just intercange disable by the enable directive.

If we need to know which resource groups and resources available in the cluster we can execute.

[source, bash]
------------------------
pcs resource group list

apachegroup: apache_lvm ha_apache_fs apache_virtualIP new_app
------------------------

=== Managing the cluster

After the cluster is configured and running there are some actions that we could execute.

==== Check Health

===== Cluster

[source, bash]
------------------------
pcs cluster status

Cluster Status:
 Cluster Summary:
   * Stack: corosync
   * Current DC: ha1.test.com (version 2.0.3-5.el8_2.1-4b1f869f0f) - partition with quorum
   * Last updated: Wed Oct 28 07:24:20 2020
   * Last change:  Tue Oct 27 15:25:40 2020 by root via cibadmin on ha1.test.com
   * 2 nodes configured
   * 5 resource instances configured
 Node List:
   * Online: [ ha1.test.com ha2.test.com ]

PCSD Status:
  ha2.test.com: Online
  ha1.test.com: Online
------------------------

===== Resources

[source, bash]
------------------------
pcs resource status

  * Resource Group: apachegroup:
    * apache_lvm        (ocf::heartbeat:LVM-activate):  Started ha2.test.com
    * ha_apache_fs      (ocf::heartbeat:Filesystem):    Started ha2.test.com
    * apache_virtualIP  (ocf::heartbeat:IPaddr2):       Started ha2.test.com
    * new_app   (ocf::heartbeat:apache):        Started ha2.test.com
------------------------

==== Managing nodes

===== Make a node to be standby
Put specified node(s) into standby mode (the node specified will no longer be able to host resources), if no nodes or options are specified the current node will be put into standby mode, if --all is specified all nodes will be put into standby mode.

[source, bash]
------------------------
pcs node standby ha2.test.com
------------------------

===== Return a node to be operative from standby state

Remove node(s) from standby mode (the node specified will now be able to host resources), if no nodes or options are specified the current node will be removed from standby mode, if --all is specified all nodes will be removed from standby mode.

[source, bash]
------------------------
pcs node unstandby ha2.test.com
------------------------

===== Make a node to be in maintenance

Put specified node(s) into maintenance mode, if no nodes or options are specified the current node will be put into maintenance mode, if --all is specified all nodes will be put into maintenance mode. If --wait is specified, pcs will wait up to 'n' seconds for the node(s) to be put into maintenance mode and then return 0 on success or 1 if the operation not succeeded yet. If 'n' is not specified it defaults to 60 minutes.

[source, bash]
------------------------
pcs node maintenance ha2.test.com
------------------------

For return the node to a operative state just use unmaintenance.

==== Managing the cluster

===== Start or stop a cluster 

A cluster can be started or stopped with a node granularity. This action starts or stops all the process required for a node to be in the cluster operative or not.

Some examples below.

[source, bash]
------------------------
pcs cluster start --all
------------------------

[source, bash]
------------------------
pcs cluster stop --all
------------------------

[source, bash]
------------------------
pcs cluster start ha1.test.com
------------------------

[source, bash]
------------------------
pcs cluster stop ha2.test.com
------------------------

===== Enable or disable a cluster node for starting at boot time.

A cluster also can be instructed to start each node at boot time.

This example enable all node.

[source, bash]
------------------------
pcs cluster enable --all
------------------------

If we need a cluster node to not joing the cluster at boot time just use "disable"

This example operates on node ha1.test.com

[source, bash]
------------------------
pcs cluster disbale ha1.test.com
------------------------







